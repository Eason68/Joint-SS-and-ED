import torch
import torch.nn as nn
import torch.nn.functional as F
from utils import knn_point, index_points


class Loss(nn.Module):
    def __init__(self):
        """
        Initialization, setting the number of samples and temperature
        """
        super(Loss, self).__init__()
        self.nsamples = [64, 32, 16, 8, 4]
        self.temperature = 1.0

    def stats_boundary_loss(self, coords, labels, feats, num_classes):
        """
        Calculate the boundary loss function: L_cbl
        :param coords: the original and 4 down sampling coordinates, which are used to calculate the neighborhood
        :param labels: the original label
        :param feats: generated by points in decoder via MLP, 32 channels
        :param num_classes: semantic categories
        :return: the boundary loss
        """
        loss = torch.tensor(.0).to(labels.device)
        labels = F.one_hot(labels, num_classes)  # [B, N, 13]
        for i, (coord, feat) in enumerate(zip(coords, feats)):

            # Generating scene labels
            if i == 0:
                probability_labels = labels
            else:
                kneighbors = i * 4  # Number of neighbours, denoted by K, K = 1, 4, 16, 64, 256
                neighbor_indexs = knn_point(kneighbors, coords[0], coord)  # [B, S, K], S: number of coord
                probability_labels = index_points(labels, neighbor_indexs)  # [B, S, K, 13]
                probability_labels = probability_labels.float().mean(dim=-2)  # [B, S, 13]

            # Select nsample neighbours for labels and features
            nsample = self.nsamples[i]
            neighbor_indexs = knn_point(nsample, coord, coord)  # [B, S, nsample], S: number of coord
            neighbor_labels = index_points(probability_labels, neighbor_indexs)  # [B, S, K, 13]
            neighbor_feats = index_points(feat, neighbor_indexs)  # [B, S, K, C]

            # Find boundary points
            center_label = torch.argmax(torch.unsqueeze(probability_labels, -2), -1)  # [B, S, 1, 13] -> [B, S, 1]
            neighbor_labels = torch.argmax(neighbor_labels, -1)  # [B, S, K]
            mask = center_label == neighbor_labels  # [B, S, K], whether each point and its neighbours are labelled the same
            point_mask = torch.sum(mask.int(), -1)  # [B, S], how many neighbours at each point have the same label
            point_mask = (point_mask > 0) & (point_mask < nsample)  # [B, S], whether a boundary point

            # If there is no boundary point, go directly to the next stage
            if not torch.any(point_mask):
                continue

            # Obtaining boundary point features
            mask = mask[point_mask]  # [B, sub_S, K]
            feat = feat[point_mask]  # [B, sub_S, C]
            neighbor_feats = neighbor_feats[point_mask]  # [B, sub_S, K, C]

            # Calculate the boundary loss
            dist = torch.unsqueeze(feat, -2) - neighbor_feats  # [B, sub_S, 1, C] - [B, sub_S, K, C]
            dist = torch.sqrt(torch.sum(dist ** 2, axis=-1) + 1e-6)  # [B, sub_S, K, C] -> [B, sub_S, K]
            dist = - dist
            dist = dist - torch.max(dist, -1, keepdim=True)[0]
            dist = dist / self.temperature

            exp = torch.exp(dist)  # [B, sub_S, K]
            pos = torch.sum(exp * mask.float())
            neg = torch.sum(exp)

            loss = loss - torch.log(pos / (neg + 1e-6))

        return loss

    def stats_ordinary_loss(self, labels, indexs, preds, num_classes):
        """
        Calculate the loss of the 5 predicted results from the decoder output
        :param labels: the original label
        :param indexs: index of downsampled points, [B, N/4], ... , [B, N/256]
        :param preds: Predicted labels at each stage in decoder, [B, N, 13], ... , [B, N/64, 13], [B, N/256, 13]
        :param num_classes: semantic categories
        :return: cross entropy loss for 5 predicted outcomes
        """
        pred1, pred2, pred3, pred4, pred5 = preds
        label2, label3, label4, label5 = [labels.gather(dim=1, index=indexs[i]) for i in range(len(indexs))]

        loss1 = F.cross_entropy(pred1.contiguous().view(-1, num_classes), labels.contiguous().view(-1))
        loss2 = F.cross_entropy(pred2.contiguous().view(-1, num_classes), label2.contiguous().view(-1))
        loss3 = F.cross_entropy(pred3.contiguous().view(-1, num_classes), label3.contiguous().view(-1))
        loss4 = F.cross_entropy(pred4.contiguous().view(-1, num_classes), label4.contiguous().view(-1))
        loss5 = F.cross_entropy(pred5.contiguous().view(-1, num_classes), label5.contiguous().view(-1))

        loss = loss1 + loss2 + loss3 + loss4 + loss5
        return loss

    def forward(self, labels, indexs, output, preds, coords, feats, num_classes=13, alpha=0.05, beta=0.05):
        """
        :param labels: the original label, B x N
        :param indexs: the set of points sampled for each downsampling, [index2, index3, index4, index5]
        :param output: prediction results of the model, B x N x 13
        :param preds: prediction labels of 5 stage in decoder, [pred1, pred2, pred3, pred4, pred5]
        :param coords: the original and 4 down sampling coordinates, [xyz1, xyz2, xyz3, xyz4, xyz5]
        :param feats: features of 5 stage in decoder, [feat1, feat2, feat3, feat4, feat5]
        :param num_classes: semantic categories
        :param alpha: weighting of boundary loss
        :param beta: weighting of loss for 5 predicted outcomes
        :return: total loss of the model
        """
        output_loss = F.cross_entropy(output.contiguous().view(-1, num_classes), labels.contiguous().view(-1))
        boundary_loss = self.stats_boundary_loss(coords, labels, feats, num_classes)
        ordinary_loss = self.stats_ordinary_loss(labels, indexs, preds, num_classes)

        loss = output_loss + alpha * boundary_loss + beta * ordinary_loss
        return loss
